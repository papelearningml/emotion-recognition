{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Importation des bibliothèques nécessaires**\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# **Fixer les graines aléatoires pour assurer la reproductibilité**\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# **Vérifier si un GPU est disponible et définir le dispositif**\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# **Définition de la fonction pour augmenter les données**\n",
    "def augmented_melspec(melspec):\n",
    "    \"\"\"\n",
    "    Applique des transformations d'augmentation de données à un spectrogramme mél.\n",
    "    \"\"\"\n",
    "    data_generator = ImageDataGenerator(\n",
    "        width_shift_range=0.05,\n",
    "        zoom_range=0.05,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    melspec_redimensionne = melspec.reshape((1,) + melspec.shape + (1,))\n",
    "    augmented_melspec = next(data_generator.flow(melspec_redimensionne, batch_size=1))\n",
    "    return augmented_melspec[0, :, :, 0]\n",
    "\n",
    "# **Chargement des données**\n",
    "hap = pd.read_csv(\"aligned_hap_database.csv\")\n",
    "ang = pd.read_csv(\"aligned_ang_database.csv\")\n",
    "neu = pd.read_csv(\"aligned_neu_database.csv\")\n",
    "sad = pd.read_csv(\"aligned_sad_database.csv\")\n",
    "\n",
    "print('Shape hap:', hap.shape)\n",
    "print('Shape ang:', ang.shape)\n",
    "print('Shape neu:', neu.shape)\n",
    "print('Shape sad:', sad.shape)\n",
    "\n",
    "# **Préparation des données**\n",
    "X = np.concatenate((hap.to_numpy(), ang.to_numpy(), neu.to_numpy(), sad.to_numpy()))\n",
    "X = X[:, 1:5201]\n",
    "X = X.reshape((-1, 130, 40))\n",
    "print('Shape X :', X.shape)\n",
    "\n",
    "Y = np.concatenate((\n",
    "    np.zeros(hap.shape[0]),\n",
    "    np.ones(ang.shape[0]),\n",
    "    np.full(neu.shape[0], 2),\n",
    "    np.full(sad.shape[0], 3)\n",
    "))\n",
    "Y = Y.astype(int)\n",
    "print('Shape Y:', Y.shape)\n",
    "\n",
    "# **Définition des classes pour le modèle**\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(d))\n",
    "        self.beta = nn.Parameter(torch.zeros(d))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True) + self.eps\n",
    "        return self.alpha * (x - mean) / std + self.beta\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_d, out_d, hidden, dropout=0.1, activation=F.relu):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.sigma = activation\n",
    "        dim = [in_d] + hidden + [out_d]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dim[i], dim[i+1]) for i in range(len(dim)-1)\n",
    "        ])\n",
    "        self.ln = nn.ModuleList([\n",
    "            LayerNorm(dim[i+1]) for i in range(len(dim)-2)\n",
    "        ])\n",
    "        self.dp = nn.ModuleList([\n",
    "            nn.Dropout(dropout) for _ in range(len(dim)-2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.ln[i](x)\n",
    "            x = self.sigma(x)\n",
    "            x = self.dp[i](x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "def _inner_product(f1, f2, h):\n",
    "    prod = f1 * f2\n",
    "    return torch.matmul((prod[:, :-1] + prod[:, 1:]) / 2, h.unsqueeze(-1))\n",
    "\n",
    "def _l1(f, h):\n",
    "    return _inner_product(torch.abs(f), torch.ones_like(f), h)\n",
    "\n",
    "def _l2(f, h):\n",
    "    return torch.sqrt(_inner_product(f, f, h))\n",
    "\n",
    "class AdaFNN(nn.Module):\n",
    "    def __init__(self, n_base, base_hidden, grid, dropout, lambda1, lambda2, device):\n",
    "        super(AdaFNN, self).__init__()\n",
    "        self.n_base = n_base\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.device = device\n",
    "\n",
    "        grid = np.array(grid)\n",
    "        self.t = torch.tensor(grid, dtype=torch.float32).to(device)\n",
    "        self.h = torch.tensor(grid[1:] - grid[:-1], dtype=torch.float32).to(device)\n",
    "\n",
    "        self.BL = nn.ModuleList([\n",
    "            FeedForward(1, 1, base_hidden, dropout, activation=F.selu)\n",
    "            for _ in range(n_base * 40)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, J, m = x.size()\n",
    "        T = self.t.unsqueeze(-1)\n",
    "\n",
    "        self.bases = [basis(T).transpose(-1, -2) for basis in self.BL]\n",
    "        l2_norm = _l2(torch.cat(self.bases, dim=0), self.h).detach()\n",
    "        self.normalized_bases = [\n",
    "            self.bases[i] / (l2_norm[i, 0] + 1e-6)\n",
    "            for i in range(self.n_base * 40)\n",
    "        ]\n",
    "\n",
    "        scores = []\n",
    "        for i in range(m):\n",
    "            basis_scores = torch.cat([\n",
    "                _inner_product(\n",
    "                    b.repeat((B, 1)),\n",
    "                    x[:, :, i],\n",
    "                    self.h\n",
    "                ) for b in self.bases[i * self.n_base:(i + 1) * self.n_base]\n",
    "            ], dim=-1)\n",
    "            scores.append(basis_scores)\n",
    "        score = torch.cat(scores, dim=-1)\n",
    "        return score\n",
    "\n",
    "    def R1(self, l1_k):\n",
    "        if self.lambda1 == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        selected = np.random.choice(self.n_base * 40, min(l1_k, self.n_base * 40), replace=False)\n",
    "        selected_bases = torch.cat([self.normalized_bases[i] for i in selected], dim=0)\n",
    "        return self.lambda1 * torch.mean(_l1(selected_bases, self.h))\n",
    "\n",
    "    def R2(self, l2_pairs):\n",
    "        if self.lambda2 == 0 or self.n_base == 1:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        k = min(l2_pairs, self.n_base * 40 * (self.n_base * 40 - 1) // 2)\n",
    "        f1, f2 = [], []\n",
    "        for _ in range(k):\n",
    "            a, b = np.random.choice(self.n_base * 40, 2, replace=False)\n",
    "            f1.append(self.normalized_bases[a])\n",
    "            f2.append(self.normalized_bases[b])\n",
    "        f1 = torch.cat(f1, dim=0)\n",
    "        f2 = torch.cat(f2, dim=0)\n",
    "        return self.lambda2 * torch.mean(torch.abs(_inner_product(f1, f2, self.h)))\n",
    "\n",
    "class MultiAda(nn.Module):\n",
    "    def __init__(self, n_base, base_hidden, grid, dropout, lambda1, lambda2, device):\n",
    "        super(MultiAda, self).__init__()\n",
    "        self.n_base = n_base\n",
    "        self.device = device\n",
    "        self.sigma = F.relu\n",
    "        self.ln = LayerNorm(40)\n",
    "\n",
    "        self.Ada = AdaFNN(n_base, base_hidden, grid, dropout, lambda1, lambda2, device)\n",
    "        self.lstm = nn.LSTM(40, 40, batch_first=True)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=40, nhead=4, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=2)\n",
    "        self.FF = FeedForward(self.n_base * 40, 4, [self.n_base * 40, self.n_base * 40, 128])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.sigma(x)\n",
    "        score = self.Ada(x)\n",
    "        out = self.FF(score)\n",
    "        return out\n",
    "\n",
    "    def L1(self, l1_k):\n",
    "        return self.Ada.R1(l1_k)\n",
    "\n",
    "    def L2(self, l2_pairs):\n",
    "        return self.Ada.R2(l2_pairs)\n",
    "\n",
    "# **Préparation du DataLoader personnalisé**\n",
    "class DataLoaderCustom:\n",
    "    def __init__(self, batch_size, n_augmented, X, Y, T, split=(8, 1, 1), random_seed=42):\n",
    "        self.n, J, m = X.shape\n",
    "        self.t = T[0]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Diviser les données en ensembles d'entraînement, de validation et de test\n",
    "        train_n = self.n * split[0] // sum(split)\n",
    "        valid_n = self.n * split[1] // sum(split)\n",
    "        test_n = self.n - train_n - valid_n\n",
    "\n",
    "        # Mélanger les données\n",
    "        np.random.seed(random_seed)\n",
    "        indices = np.random.permutation(self.n)\n",
    "        X = X[indices]\n",
    "        Y = Y[indices]\n",
    "\n",
    "        # Split des données\n",
    "        self.train_X = X[:train_n]\n",
    "        self.train_Y = Y[:train_n]\n",
    "        self.valid_X = X[train_n:train_n + valid_n]\n",
    "        self.valid_Y = Y[train_n:train_n + valid_n]\n",
    "        self.test_X = X[train_n + valid_n:]\n",
    "        self.test_Y = Y[train_n + valid_n:]\n",
    "\n",
    "        # Augmentation des données d'entraînement\n",
    "        aug_X, aug_Y = [], []\n",
    "        for i in range(len(self.train_X)):\n",
    "            for _ in range(n_augmented):\n",
    "                augmented = augmented_melspec(self.train_X[i])\n",
    "                aug_X.append(augmented)\n",
    "                aug_Y.append(self.train_Y[i])\n",
    "        self.train_X = np.concatenate((self.train_X, np.array(aug_X)))\n",
    "        self.train_Y = np.concatenate((self.train_Y, np.array(aug_Y)))\n",
    "\n",
    "        # Mise à l'échelle des données\n",
    "        self.scaler = StandardScaler()\n",
    "        self.train_X = self.scaler.fit_transform(self.train_X.reshape(-1, m)).reshape(-1, J, m)\n",
    "        self.valid_X = self.scaler.transform(self.valid_X.reshape(-1, m)).reshape(-1, J, m)\n",
    "        self.test_X = self.scaler.transform(self.test_X.reshape(-1, m)).reshape(-1, J, m)\n",
    "\n",
    "        # Calcul du nombre de lots\n",
    "        self.train_batches = len(self.train_X) // batch_size\n",
    "        self.valid_batches = len(self.valid_X) // batch_size\n",
    "        self.test_batches = len(self.test_X) // batch_size\n",
    "\n",
    "    def shuffle(self):\n",
    "        # Mélanger les données d'entraînement\n",
    "        indices = np.random.permutation(len(self.train_X))\n",
    "        self.train_X = self.train_X[indices]\n",
    "        self.train_Y = self.train_Y[indices]\n",
    "\n",
    "    def get_train_batch(self):\n",
    "        for i in range(self.train_batches):\n",
    "            x = self.train_X[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.train_Y[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            yield torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def get_valid_batch(self):\n",
    "        for i in range(self.valid_batches):\n",
    "            x = self.valid_X[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.valid_Y[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            yield torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def get_test_batch(self):\n",
    "        for i in range(self.test_batches):\n",
    "            x = self.test_X[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.test_Y[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            yield torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# **Configuration des hyperparamètres**\n",
    "n_augmented = 3\n",
    "batch_size = 128\n",
    "split = (80, 10, 10)\n",
    "T = [np.linspace(0, 1, 130)]\n",
    "grid = T[0].tolist()\n",
    "\n",
    "# **Initialisation du DataLoader personnalisé**\n",
    "dataLoader = DataLoaderCustom(batch_size, n_augmented, X, Y, T, split)\n",
    "\n",
    "# **Définition du modèle**\n",
    "base_hidden = [256, 256, 256, 256]\n",
    "n_base = 6\n",
    "lambda1, l1_k = 0.0, 2\n",
    "lambda2, l2_pairs = 0.5, 3\n",
    "dropout = 0.1\n",
    "model = MultiAda(\n",
    "    n_base=n_base,\n",
    "    base_hidden=base_hidden,\n",
    "    grid=grid,\n",
    "    dropout=dropout,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    "    device=device\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# **Configuration de l'entraînement**\n",
    "epochs = 100\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# **Boucle d'entraînement**\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "early_stopping_patience = 10\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    dataLoader.shuffle()\n",
    "    batch_losses = []\n",
    "\n",
    "    for x_batch, y_batch in dataLoader.get_train_batch():\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss += model.L1(l1_k) + model.L2(l2_pairs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(batch_losses)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in dataLoader.get_valid_batch():\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            outputs = model(x_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    valid_loss = np.mean(val_losses)\n",
    "    valid_losses.append(valid_loss)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# **Chargement du meilleur modèle**\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# **Évaluation du modèle sur le jeu de test**\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in dataLoader.get_test_batch():\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        outputs = model(x_test)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(y_test.cpu().numpy())\n",
    "\n",
    "# **Calcul des métriques**\n",
    "print(\"Classification Report:\")\n",
    "target_names = ['Happy', 'Angry', 'Neutral', 'Sad']\n",
    "print(classification_report(test_labels, test_preds, target_names=target_names))\n",
    "\n",
    "# **Affichage de la matrice de confusion**\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# **Affichage des courbes de perte**\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# **Sauvegarde du modèle entraîné**\n",
    "torch.save(model, 'emotion_recognition_model.pth')\n",
    "\n",
    "# **Sauvegarde des courbes de perte**\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig('loss_curves.png')\n",
    "\n",
    "print(\"Training complete. Model saved as 'emotion_recognition_model.pth'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
